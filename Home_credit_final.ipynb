{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A Data Science Project on Credit Default Risk Analysis**\n",
    "##### Author:Mukesh Kumar Chaudhary\n",
    "##### Email:cmukesh8688@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Statement**\n",
    "Home Credit B.V. is an international non-bank financial institution founded in 1997 in the Czech Republic.The company operates in 14 countries and focuses on lending primarily to people with little or no credit history. As of 2016 the company has over 15 million active customers, with two-thirds of them in Asia and 7.3 million in China. \n",
    "\n",
    "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n",
    "\n",
    "Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data**\n",
    "\n",
    "\n",
    "- application_{train|test}.csv\n",
    "\n",
    " - This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n",
    " - Static data for all applications. One row represents one loan in our data sample.\n",
    " \n",
    " \n",
    "- bureau.csv\n",
    " - All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a      loan in our sample).\n",
    " - For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n",
    " \n",
    " \n",
    "- bureau_balance.csv\n",
    " - Monthly balances of previous credits in Credit Bureau.\n",
    " - This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in      sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n",
    " \n",
    "\n",
    "- POS_CASH_balance.csv\n",
    " - Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n",
    " - This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to    loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some        history observable for the previous credits) rows.\n",
    " \n",
    " \n",
    "- credit_card_balance.csv\n",
    " - Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n",
    " - This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to    loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some      history observable for the previous credit card) rows.\n",
    " \n",
    " \n",
    "- previous_application.csv\n",
    " - All previous applications for Home Credit loans of clients who have loans in our sample.\n",
    " - There is one row for each previous application related to loans in our data sample.\n",
    " \n",
    " \n",
    "- installments_payments.csv\n",
    " - Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n",
    "   There is a) one row for every payment that was made plus b) one row each for missed payment.\n",
    "   One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit    credit related to loans in our sample.\n",
    "   \n",
    "\n",
    "- train_bureau.csv \n",
    " - This dataframe is created manualy by group joining application_train,bureau and bureau_balance dataframe with aggregation funtion count,sum,max,min,mean .\n",
    " \n",
    " \n",
    "- previous_loan_final.csv\n",
    " -  This dataframe is created manually by group joining previous_application,POS_CASH_balance,credi_card_balance and intallments_payments dataframe with aggregation funtion count,sum,max,min,mean .\n",
    " \n",
    " \n",
    "- home_credit_final.csv\n",
    " - This dataframe is created manually by joining train_bureau.csv and previous_loan_final.csv. \n",
    " \n",
    " \n",
    "- automative_features_app.csv\n",
    " - This is created by auto generated library ***featuretools*** with aggregation premitives sum,max,min,mode,mean,count\n",
    " \n",
    " \n",
    "![image](image/database_flowchart.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "# for data mauplation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ignore warnig from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# for featuretools\n",
    "import featuretools as ft\n",
    "\n",
    "# import user libraries \n",
    "from text_format_class import TxtFormat \n",
    "import manage_missing_data as manage_df\n",
    "import manage_dataframe as manage_agg_cat\n",
    "import display_corr as manage_corr\n",
    "import manage_model\n",
    "import manage_pca\n",
    "import plot_features\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn library for preprocessing ,modelling , Accuracy Analysis , Cross Validation , optimization \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "# XGBoosting \n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Gradient Boosting algorithm\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Data and Data dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all Data \n",
    "pd.options.display.max_columns = None\n",
    "df_app_train = pd.read_csv(\"Data/application_train.csv\")\n",
    "df_app_test = pd.read_csv(\"Data/application_test.csv\")\n",
    "df_bureau = pd.read_csv(\"Data/bureau.csv\")\n",
    "df_bureau_balance = pd.read_csv(\"Data/bureau_balance.csv\")\n",
    "df_previous = pd.read_csv(\"Data/previous_application.csv\")\n",
    "df_credit = pd.read_csv(\"Data/credit_card_balance.csv\")\n",
    "df_cash = pd.read_csv(\"Data/POS_CASH_balance.csv\")\n",
    "df_payment = pd.read_csv(\"Data/installments_payments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of Data \n",
    "print(\"Dimension of Data\")\n",
    "print(\"------------------\")\n",
    "print(\"Application Train    : \", df_app_train.shape )\n",
    "print(\"Application Test     :\",df_app_test.shape)\n",
    "print(\"Bureau               :\" ,df_bureau.shape)\n",
    "print(\"Bureau Balance       :\",df_bureau_balance.shape)\n",
    "print(\"Previous application : \",df_previous.shape)\n",
    "print(\"Credit card balance  :\",df_credit.shape)\n",
    "print(\"POSH_CASH_balance    :\" ,df_cash.shape)\n",
    "print(\"Instalments payment  :\",df_payment.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application_train from Home credit\n",
    "print(df_app_train.shape)\n",
    "df_app_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test application\n",
    "print(df_app_test.shape)\n",
    "df_app_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns names between application test and application train \n",
    "for col in df_app_train.columns:\n",
    "    if col not in df_app_test.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_columns = None\n",
    "print(df_app_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit history from another bureau\n",
    "print(df_bureau.shape)\n",
    "df_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of bureau dataset\n",
    "df_bureau.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bureau balance \n",
    "print(df_bureau_balance.shape)\n",
    "df_bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of bureau balance dataset\n",
    "df_bureau_balance.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous applicattion \n",
    "print(df_previous.shape)\n",
    "df_previous.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of previous application \n",
    "df_previous.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit card balance dataset\n",
    "print(df_credit.shape)\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of credit card balance dataset\n",
    "df_credit.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSH_CASH balance dataset \n",
    "print(df_cash.shape)\n",
    "df_cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of POSH_CASH balance dataset\n",
    "df_cash.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalments payments dataset \n",
    "print(df_payment.shape)\n",
    "df_payment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of instalments_payments \n",
    "df_payment.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application train\n",
    "manage_df.missing_data_display(df_app_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete missing value more than 40% \n",
    "manage_df.delete_missing_values(df_app_train)\n",
    "manage_df.handle_missing_value(df_app_train)\n",
    "manage_df.missing_data_display(df_app_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#application test \n",
    "manage_df.delete_missing_values(df_app_test)\n",
    "manage_df.handle_missing_value(df_app_test)\n",
    "manage_df.missing_data_display(df_app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bureau dataset\n",
    "manage_df.delete_missing_values(df_bureau)\n",
    "manage_df.handle_missing_value(df_bureau)\n",
    "manage_df.missing_data_display(df_bureau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bureu balance dataset \n",
    "manage_df.delete_missing_values(df_bureau_balance)\n",
    "manage_df.handle_missing_value(df_bureau_balance)\n",
    "manage_df.missing_data_display(df_bureau_balance)\n",
    "print(df_bureau_balance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous application dataset\n",
    "print(df_previous.shape)\n",
    "manage_df.delete_missing_values(df_previous)\n",
    "manage_df.handle_missing_value(df_previous)\n",
    "print(df_previous.shape)\n",
    "manage_df.missing_data_display(df_previous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POSh_CASH dataset \n",
    "print(df_cash.shape)\n",
    "manage_df.delete_missing_values(df_cash)\n",
    "manage_df.handle_missing_value(df_cash)\n",
    "print(df_cash.shape)\n",
    "manage_df.missing_data_display(df_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit balance dataset \n",
    "print(df_credit.shape)\n",
    "manage_df.delete_missing_values(df_credit)\n",
    "manage_df.handle_missing_value(df_credit)\n",
    "print(df_credit.shape)\n",
    "manage_df.missing_data_display(df_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#payment balance dataset \n",
    "print(df_payment.shape)\n",
    "manage_df.delete_missing_values(df_payment)\n",
    "manage_df.handle_missing_value(df_payment)\n",
    "print(df_payment.shape)\n",
    "manage_df.missing_data_display(df_payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About Application train dataframe\n",
    "\n",
    "print(df_app_train.shape)\n",
    "df_app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of df_application_train \n",
    "df_app_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating percentage of not repaid loan and ploting \n",
    "\n",
    "total_no_applicant = df_app_train['TARGET'].count()\n",
    "\n",
    "total_repaid = df_app_train[df_app_train['TARGET']==0]['TARGET'].count()\n",
    "\n",
    "total_not_repaid = df_app_train[df_app_train['TARGET']==1]['TARGET'].count()\n",
    "\n",
    "\n",
    "\n",
    "start = \"\\033[1m\"\n",
    "end = \"\\033[0;0m\"\n",
    "\n",
    "print(\"Status of Target\")\n",
    "print(\"------------------\")\n",
    "print(f\"total_no_applicant : {TxtFormat().BOLD} {total_no_applicant} {TxtFormat().END} \")\n",
    "print(f\"\"\"total loan was repaid: {TxtFormat().BOLD} {total_repaid} {TxtFormat().END} and Percent :  {TxtFormat().BOLD} {round(total_repaid/total_no_applicant *100)} %  {TxtFormat().END} \"\"\")\n",
    "print(f\"total loan was not repaid: {TxtFormat().BOLD} {total_not_repaid} {TxtFormat().END} and Percent : {TxtFormat().BOLD} {round(total_not_repaid/total_no_applicant *100)} %  {TxtFormat().END} \")\n",
    "\n",
    "\n",
    "# plot graph\n",
    "\n",
    "fig , ax = plt.subplots(nrows=1,ncols=2,figsize=(16,5))\n",
    "df_app_train['TARGET'].value_counts().plot(kind='bar' ,colors = sns.color_palette(), ax =ax[0], fontsize = 14 , label = '0: Repaid \\n 1: Notrepaid')\n",
    "ax[0].set_title(\"Count of target variable\",fontsize = 14)\n",
    "ax[0].set_ylabel(\"Counts\", fontsize = 14)\n",
    "ax[0].set_xlabel(\"Target\",fontsize = 14)\n",
    "\n",
    "df_app_train['TARGET'].value_counts().plot.pie(autopct = \"%1.0f%%\", colors = sns.color_palette(), labels = ['repaid','notrepaid'],fontsize =18 ,ax =ax[1])\n",
    "ax[1].set_title(\"Distribution of target variable\",fontsize = 14)\n",
    "\n",
    "fig.savefig(\" Target Level.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status of NAME_CONTRACT_TYPE with target\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','NAME_CONTRACT_TYPE','SK_ID_CURR',horizantal= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender with respect to target\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','CODE_GENDER','SK_ID_CURR',horizantal= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status CNT_CHILDREN\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','CNT_CHILDREN','SK_ID_CURR',horizantal= False)\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_TYPE_SUITE\n",
    "\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','NAME_TYPE_SUITE','SK_ID_CURR',horizantal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_INCOME_TYPE  feature plot with target \n",
    "\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','NAME_INCOME_TYPE','SK_ID_CURR',horizantal=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems working income type percetage on both target (0 , 1) has high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family status\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','NAME_FAMILY_STATUS','SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status of NAME_HOUSING_TYPE\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','NAME_HOUSING_TYPE','SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status of OCCUPATION_TYPE\n",
    "\n",
    "plot_features.display_targetfeature(df_app_train,'TARGET','OCCUPATION_TYPE','SK_ID_CURR',horizantal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of Amount of Credit\n",
    "plot_features.plot_distribution_feature(df_app_train,'AMT_CREDIT','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features.plot_distribution_feature(df_app_train,'AMT_INCOME_TOTAL','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check outlier \n",
    "\n",
    "#df_app_train['AMT_INCOME_TOTAL'].boxplot()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df_app_train[(np.abs(stats.zscore(df_app_train)) < 3).all(axis=1)]\n",
    "\n",
    "#np.abs(stats.zscore(df_app_train['AMT_INCOME_TOTAL']) < 3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMT_ANNUITY\n",
    "plot_features.plot_distribution_feature(df_app_train,'AMT_ANNUITY','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   AMT_GOODS_PRICE\n",
    "\n",
    "plot_features.plot_distribution_feature(df_app_train,'AMT_GOODS_PRICE','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features.plot_distribution_feature(df_app_train,'DAYS_BIRTH','blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The negative value means that date pf birth is in past . The age range \n",
    " is between approximative 20 to 68   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day employed distribution \n",
    "plot_features.plot_distribution_feature(df_app_train,'DAYS_EMPLOYED','blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negetive value means unemployed but it's not clear. Most of people employed more than 100 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days of registration distribution\n",
    "plot_features.plot_distribution_feature(df_app_train,'DAYS_REGISTRATION','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kde plot EXT_SOURCE_3\n",
    "\n",
    "manage_corr.Kde_target('EXT_SOURCE_3',df_app_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_ID_PUBLISH']\n",
    "\n",
    "for v in var:\n",
    "    a = str(v).split()\n",
    "    print(a)\n",
    "    \n",
    "a = 'Mukesh'\n",
    "\n",
    "print(a.split())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare with target = 1 and target = 0 with \n",
    "# var = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_ID_PUBLISH']\n",
    "\n",
    "\n",
    "var = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_ID_PUBLISH']\n",
    "plot_features.plot_distribution_comp(df_app_train,var,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check top 10 features correlation with target\n",
    "\n",
    "manage_corr.target_corrs(df_app_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA when join application train and previous loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df_app_train and df_previous \n",
    "\n",
    "df_train_previous_eda = df_app_train.merge(df_previous,on='SK_ID_CURR',how ='left')\n",
    "print(df_train_previous_eda.shape)\n",
    "df_train_previous_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of joining application and previous application loan\n",
    "df_train_previous_eda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status of 'NAME_CONTRACT_STATUS' of previous loan\n",
    "\n",
    "plot_features.display_targetfeature(df_train_previous_eda,'TARGET','NAME_CONTRACT_STATUS','SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'AMT_CREDIT_y' of previous loan\n",
    "\n",
    "plot_features.plot_distribution_onefeature(df_train_previous_eda,'AMT_CREDIT_y',color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kde plot AMT_CREDIT_y\n",
    "\n",
    "manage_corr.Kde_target('AMT_CREDIT_y',df_train_previous_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare of previous loan 'AMT_ANNUITY_y', 'AMT_APPLICATION' ,'AMT_CREDIT_y', 'AMT_GOODS_PRICE_y'\n",
    "\n",
    "features = ['AMT_ANNUITY_y', 'AMT_APPLICATION','AMT_CREDIT_y', 'AMT_GOODS_PRICE_y']\n",
    "\n",
    "plot_features.plot_distribution_comp(df_train_previous_eda,features,n_row=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bureau data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension and info\n",
    "df_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit amount distribution \n",
    "\n",
    "plot_features.plot_distribution_onefeature(df_bureau,'AMT_CREDIT_SUM','blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "previou_loan_final.csv is created by aggregation joining  previous application , POS_CASH_Balance , Intallment payment  and credit_balance .This dataframe has information of previous loan with transactions of cash and credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous loan application information which has credit amount , status , type of loan \n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous['NAME_CONTRACT_TYPE'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous[df_previous['NAME_CONTRACT_TYPE']=='Revolving loans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicated as column_wise for analysis one-many ralationship\n",
    "print(df_previous['SK_ID_PREV'].duplicated().sum())\n",
    "print(df_previous['SK_ID_CURR'].duplicated().sum())\n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking  each row as duplicated \n",
    "df_previous.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cash['SK_ID_PREV'].duplicated().sum())\n",
    "print(df_cash['SK_ID_CURR'].duplicated().sum())\n",
    "df_cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_credit['SK_ID_PREV'].duplicated().sum())\n",
    "print(df_credit['SK_ID_CURR'].duplicated().sum())\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_payment['SK_ID_PREV'].duplicated().sum())\n",
    "print(df_payment['SK_ID_CURR'].duplicated().sum())\n",
    "df_payment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_previous[df_previous['SK_ID_PREV']==2802425])\n",
    "df_cash[df_cash['SK_ID_PREV']==2802425]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_credit[df_credit['SK_ID_PREV']==2802425]  # cash loan id\n",
    "#df_credit[df_credit['SK_ID_PREV']==2030495]  # consumer loand id \n",
    "#df_credit[df_credit['SK_ID_PREV']==1285768].sort_values(by='MONTHS_BALANCE',ascending = False)  # revolving loan id \n",
    "df_credit[df_credit['SK_ID_PREV']==1629736].sort_values(by='MONTHS_BALANCE',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment[df_payment['SK_ID_PREV']==1629736].sort_values(by='NUM_INSTALMENT_NUMBER',ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def rerurn_size(df):\n",
    "    # return size by dataframe in gigabyte\n",
    "    return round(sys.getsizeof(df)/1e9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_num = manage_agg_cat.agg_numeric(df_previous,group_var='SK_ID_CURR',df_name='previous')\n",
    "print(df_previous_num.shape)\n",
    "df_previous_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_cat = manage_agg_cat.count_categorical(df_previous,group_var='SK_ID_CURR',df_name='previous')\n",
    "print(df_previous_cat.shape)\n",
    "df_previous_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_df.missing_data_display(df_previous_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_final = df_previous_num.merge(df_previous_cat,on='SK_ID_CURR',how = 'inner')\n",
    "print(df_previous_final.shape)\n",
    "df_previous_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_df.missing_data_display(df_previous_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric data grouping by SK_ID_CURR on POS_CASH_balance\n",
    "df_cash_num = manage_agg_cat.agg_numeric(df_cash,group_var='SK_ID_CURR',df_name='pos_cash')\n",
    "print(df_cash_num.shape)\n",
    "df_cash_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data grouping by SK_ID_CURR on POS_CASH_balance\n",
    "df_cash_cat = manage_agg_cat.count_categorical(df_cash,group_var='SK_ID_CURR',df_name='pos_cash')\n",
    "print(df_cash_cat.shape)\n",
    "df_cash_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging numeric data and categorical data \n",
    "\n",
    "df_cash_final = df_cash_num.merge(df_cash_cat,on='SK_ID_CURR',how = 'inner')\n",
    "print(df_cash_final.shape)\n",
    "df_cash_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### credit_card_balance dataframe\n",
    "\n",
    "Credit_card_balance dataframe has credit transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric data grouping by SK_ID_CURR on credit_card_balance\n",
    "df_credit_num = manage_agg_cat.agg_numeric(df_credit,group_var='SK_ID_CURR',df_name='credit')\n",
    "print(df_credit_num.shape)\n",
    "df_credit_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical  data grouping by SK_ID_CURR on credit_card_balance\n",
    "df_credit_cat = manage_agg_cat.count_categorical(df_credit,group_var='SK_ID_CURR',df_name='credit')\n",
    "print(df_credit_cat.shape)\n",
    "df_credit_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging numeric data and categorical data of Credit_card_balance dataframe \n",
    "\n",
    "df_credit_final = df_credit_num.merge(df_credit_cat,on='SK_ID_CURR',how = 'inner')\n",
    "print(df_credit_final.shape)\n",
    "df_credit_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalments Payment DataFrame\n",
    "This has payment and miss payment history of previous loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric data grouping by SK_ID_CURR on intalments payment dataframe \n",
    "df_payment_num = manage_agg_cat.agg_numeric(df_payment,group_var='SK_ID_CURR',df_name='payment')\n",
    "print(df_payment_num.shape)\n",
    "df_payment_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining all dataframe df_prevoius_final, df_cash_final,df_credit_final,df_payment_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all dataframe df_previous_final, df_cash_final,df_credit_final,df_payment_num\n",
    "\n",
    "df_previous_loan = df_previous_final.merge(df_cash_final,on='SK_ID_CURR',how ='left')\n",
    "\n",
    "\n",
    "\n",
    "df_previous_loan = df_previous_loan.merge(df_credit_final,on='SK_ID_CURR',how ='left')\n",
    "\n",
    "\n",
    "df_previous_loan = df_previous_loan.merge(df_payment_num,on='SK_ID_CURR',how ='left')\n",
    "\n",
    "\n",
    "print(df_previous_loan.shape)\n",
    "\n",
    "df_previous_loan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values \n",
    "\n",
    "manage_df.missing_data_display(df_previous_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file in csv \n",
    "\n",
    "df_previous_loan.to_csv(\"previou_loan_final.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"previou_loan_final.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete from memory\n",
    "\n",
    "del df_test,df_previous_cat,df_previous_final,df_previous_num,df_cash_cat,df_cash_final,df_cash_num\n",
    "del df_credit_cat,df_credit_final,df_credit_num,df_payment_num\n",
    "del df_app_test,df_app_train,df_bureau,df_bureau_balance,df_cash,df_credit,df_payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Bureau Dataframe\n",
    "\n",
    "Here , train_bureau.csv dataframe is generated by aggregation join application ,bureau and bureau_balance dataframe. Bureau has client's prevoius loan which is from other institution. Model and analysis client's status in term of previous loan of other institution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about bureau \n",
    "print(df_bureau.shape)\n",
    "df_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by client id (SK_ID_CURR) and count previous loan no\n",
    "\n",
    "previous_loan_counts= df_bureau.groupby('SK_ID_CURR',as_index=False)['SK_ID_BUREAU'].count().rename(columns={'SK_ID_BUREAU':'previous_loan_counts'})\n",
    "previous_loan_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with application training dataframe \n",
    "\n",
    "df_train= df_app_train.merge(previous_loan_counts,on='SK_ID_CURR',how='left')\n",
    "print(df_train.shape)\n",
    "manage_df.missing_data_display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill 0 with null value in no of loan counts \n",
    "\n",
    "df_train.fillna(0,inplace=True)\n",
    "manage_df.missing_data_display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kde_target('EXT_SOURCE_3',df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating Numeric Columns\n",
    "\n",
    "df_bureau_agg = df_bureau.drop(columns=['SK_ID_BUREAU']).groupby('SK_ID_CURR',as_index=False).agg(['count','mean','max','min','sum']).reset_index()\n",
    "df_bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns name \n",
    "columns = ['SK_ID_CURR']\n",
    "for var in df_bureau_agg.columns.levels[0]:\n",
    "    if var !='SK_ID_CURR':\n",
    "        \n",
    "        # iterate \n",
    "        for stat in df_bureau_agg.columns.levels[1][:-1]:\n",
    "            columns.append('bureau_%s_%s'%(var,stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign columns name in groupby function\n",
    "df_bureau_agg.columns=columns\n",
    "df_bureau_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missin values \n",
    "manage_df.missing_data_display(df_bureau_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meger with training data \n",
    "\n",
    "df_train = df_train.merge(df_bureau_agg,on='SK_ID_CURR',how='left')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlations aggregated values with target \n",
    "\n",
    "# list of new correlation\n",
    "\n",
    "new_corr= []\n",
    "\n",
    "# iteration with columns\n",
    "\n",
    "for col in columns:\n",
    "    corr = df_train['TARGET'].corr(df_train[col])\n",
    "    \n",
    "    new_corr.append((col,corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the correlation with absolute value\n",
    "\n",
    "new_corr = sorted(new_corr,key = lambda x:abs(x[1]),reverse=True)\n",
    "new_corr[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kde_target('bureau_DAYS_CREDIT_mean',df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric(df,group_var,df_name):\n",
    "    \n",
    "    # remove primary id variables \n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns=col)\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    df_num = df.select_dtypes(exclude = 'object')\n",
    "    df_num[group_var] =group_ids\n",
    "    \n",
    "    \n",
    "    \n",
    "    # group by specific variable and cal statistic\n",
    "    \n",
    "    df_agg = df_num.groupby(group_var).agg(['count','mean','max','min','sum']).reset_index()\n",
    "    \n",
    "    \n",
    "    # all columns name \n",
    "    \n",
    "    columns = [group_var]\n",
    "    \n",
    "    # iteration for adding all columns name \n",
    "    \n",
    "    \n",
    "    for var in df_agg.columns.levels[0]:\n",
    "        # skip group name\n",
    "        if var != group_var:\n",
    "            \n",
    "            # iteration again\n",
    "            for stat in df_agg.columns.levels[1][:-1]:\n",
    "                columns.append('%s_%s_%s' %(df_name,var,stat))\n",
    "                \n",
    "    df_agg.columns = columns\n",
    "    return df_agg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation dataframe from Bureau dataframe  \n",
    "df_bureau_num = manage_agg_cat.agg_numeric(df_bureau,group_var='SK_ID_CURR',df_name='bureau')\n",
    "print(df_bureau_num.shape)\n",
    "df_bureau_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting SK_ID_CURR on aggregation dataframe of bureau dataframe \n",
    "df_bureau_num['SK_ID_CURR'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical dataframe from bureau dataframe \n",
    "df_bureau_cat = manage_agg_cat.count_categorical(df_bureau,group_var='SK_ID_CURR',df_name='bureau')\n",
    "print(df_bureau_cat.shape)\n",
    "df_bureau_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting SK_ID_CURR on aggregation dataframe of bureau dataframe \n",
    "df_bureau_cat['SK_ID_CURR'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging bureau_agg and  bureau_num\n",
    "\n",
    "df_bureau_final = df_bureau_num.merge(df_bureau_cat,on='SK_ID_CURR',how = 'inner')\n",
    "print(df_bureau_final.shape)\n",
    "df_bureau_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing value in new datagframe df_bureau_final\n",
    "manage_df.missing_data_display(df_bureau_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bureau Balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display of bureau balance overview\n",
    "# one row represents one month transaction \n",
    "# it has too many SK_ID_BUREAU id repeating\n",
    "\n",
    "print(df_bureau_balance.shape)\n",
    "df_bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting number of SK_ID_BUREAU, one row represent one month transaction with SK_ID_BUREAU\n",
    "df_bureau_balance['SK_ID_BUREAU'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical data on  bureau balance dataframe\n",
    "\n",
    "df_bureau_balance_cat = manage_agg_cat.count_categorical(df_bureau_balance,group_var='SK_ID_BUREAU',df_name='bureau_balance')\n",
    "print(df_bureau_balance_cat.shape)\n",
    "df_bureau_balance_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting SK_ID_BUREAU\n",
    "df_bureau_balance_cat['SK_ID_BUREAU'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical data on bureau balance \n",
    "\n",
    "df_bureau_balance_num = manage_agg_cat.agg_numeric(df_bureau_balance,group_var='SK_ID_BUREAU',df_name='bureau_balance')\n",
    "print(df_bureau_balance_num.shape)\n",
    "\n",
    "df_bureau_balance_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting SK_ID_BUREAU in numerical aggregation dataframe of bureau balance \n",
    "\n",
    "df_bureau_balance_num['SK_ID_BUREAU'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau[['SK_ID_BUREAU','SK_ID_CURR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge bureau balance agg and cat datafarame\n",
    "df_bureau_balance_final = df_bureau_balance_num.merge(df_bureau_balance_cat,on='SK_ID_BUREAU', how='inner')\n",
    "\n",
    "print(df_bureau_balance_final.shape)\n",
    "\n",
    "#merge with df_bureau dataframe where  unique SK_ID_BUREAU represents loan id \n",
    "# numbers of unique SK_ID_BUREAU id  represent no of loans\n",
    "df_bureau_by_loan = df_bureau[['SK_ID_BUREAU','SK_ID_CURR']].merge(df_bureau_balance_final,on='SK_ID_BUREAU' , how ='inner')\n",
    "\n",
    "print(df_bureau_by_loan.shape)\n",
    "df_bureau_by_loan.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again grouping new dataframe with aggregation stastics \n",
    "\n",
    "df_bureau_by_loan_final = manage_agg_cat.agg_numeric(df_bureau_by_loan,group_var='SK_ID_CURR',df_name='FinalBurBal')\n",
    "print(df_bureau_by_loan_final.shape)\n",
    "df_bureau_by_loan_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merging final bureau balance dataframe and final bureau dataframe\n",
    "\n",
    "df_bureauAndbureaubalance = df_bureau_final.merge(df_bureau_by_loan_final,on='SK_ID_CURR')\n",
    "print(df_bureauAndbureaubalance.shape)\n",
    "\n",
    "# save df_bureauAndbureaubalance dataframe into csv file \n",
    "# which came from df_bureau and df_bureau dataframe after aggrigation statistic \n",
    "#  on both numeric and categorical data type\n",
    "\n",
    "df_bureauAndbureaubalance.to_csv('BureauAndBureaubalance.csv')\n",
    "\n",
    "df_bureauAndbureaubalance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values on final dataframe which came from bureau and bureau balance\n",
    "\n",
    "manage_df.missing_data_display(df_bureauAndbureaubalance).head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### application train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation data numeric data\n",
    "df_train_num = manage_agg_cat.agg_numeric(df_app_train.drop(columns = ['TARGET']),group_var='SK_ID_CURR',df_name='train')\n",
    "print(df_train_num.shape)\n",
    "df_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining target \n",
    "\n",
    "df_train_num = df_train_num.merge(df_app_train[['SK_ID_CURR','TARGET']],on='SK_ID_CURR',how ='left')\n",
    "print(df_train_num.shape)\n",
    "df_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data for application train\n",
    "\n",
    "df_train_cat = manage_agg_cat.count_categorical(df_app_train,group_var='SK_ID_CURR',df_name='train')\n",
    "print(df_train_cat.shape)\n",
    "df_train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging numeric agg and categorical train dataframe \n",
    "\n",
    "df_train_final = df_train_num.merge(df_train_cat,on='SK_ID_CURR')\n",
    "print(df_train_final.shape)\n",
    "df_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values tain final dataframe \n",
    "\n",
    "manage_df.missing_data_display(df_train_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train final dataframe and bureau final dataframe\n",
    "\n",
    "df_train_bureau = df_train_final.merge(df_bureauAndbureaubalance,on='SK_ID_CURR',how ='left')\n",
    "print(df_train_bureau.shape)\n",
    "df_train_bureau.to_csv(\"train_bureau.csv\")\n",
    "df_train_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values in joined dataframe train and burean\n",
    "\n",
    "manage_df.missing_data_display(df_train_bureau).head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete null values which were genereted during aggregation train and joining dataframe\n",
    "manage_df.delete_missing_values(df_train_bureau)\n",
    "manage_df.missing_data_display(df_train_bureau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status of final train dataframe \n",
    "\n",
    "print(df_train_bureau.shape)\n",
    "df_train_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation between target and generated features\n",
    "\n",
    "target_corr = manage_corr.target_corrs(df_train_bureau)\n",
    "target_corr[:20]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plor kde \n",
    "\n",
    "manage_corr.Kde_target('train_CNT_CHILDREN_mean',df_train_bureau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check collinear features \n",
    "plt.figure(figsize = (12,6))\n",
    "sns.heatmap(df_train_bureau.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test dummy model on Application data only\n",
    "\n",
    "Here , Dummy model is created from only base on application dataframe to find out general information of client. It is also used for analysis features importance and other behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical datatype from application dataf\n",
    "print(len(df_app_train.select_dtypes(\"object\").columns))\n",
    "X_cat = df_app_train.select_dtypes(\"object\")\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values \n",
    "\n",
    "manage_df.missing_data_display(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy \n",
    "X_cat_dummy = pd.DataFrame(pd.get_dummies(X_cat,drop_first=True),index=X_cat.index)\n",
    "print(X_cat_dummy.shape)\n",
    "X_cat_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting numerical data for futher steps\n",
    "X_num = df_app_train.select_dtypes(exclude=\"object\")\n",
    "print(X_num.shape,df_app_train.shape)\n",
    "X_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking \n",
    "manage_df.missing_data_display(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple overview of statistic info.\n",
    "X_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na by mean values \n",
    "X_num.fillna(X_num.mean(),inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of numerical dataframe and categorical dataframe \n",
    "print(X_num.shape,X_cat_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge numerical dataframe and categorical dataframe after get_dummies of categorical data \n",
    "X_final = X_num.merge(X_cat_dummy,left_index=True,right_index=True)\n",
    "print(X_final.shape)\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 30% of subset dataframe with balance subset for model \n",
    "\n",
    "X_subset = X_final.drop(columns=['TARGET'])\n",
    "y_subset = X_final['TARGET']\n",
    "X_train_subset , X_test_subset,y_train_subset,y_test_subset=train_test_split(X_subset,y_subset,test_size=0.3,stratify=y_subset,random_state=42)\n",
    "print(X_test_subset.shape,y_test_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 30% subset data of features dataframe and target features \n",
    "X_final=X_test_subset.join(y_test_subset)\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final['TARGET'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target and features\n",
    "\n",
    "X = X_final.drop(columns=['TARGET','SK_ID_CURR'])\n",
    "y= X_final['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataframe and test data frame \n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =.25,random_state = 23)\n",
    "print(X_train.shape,X_test.shape)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension of dataframe by PCA\n",
    "# Scalling for PCA \n",
    "ss = StandardScaler()\n",
    "\n",
    "df_ss = pd.DataFrame(ss.fit_transform(X_train),columns=X_train.columns)\n",
    "\n",
    "print(df_ss.shape)\n",
    "df_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding no of components by PCA algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_ss)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "d = [n for n in range(len(cumsum))]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(d, cumsum, color='red', label = 'Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.axhline(y = 90, color='k', linestyle='--', label = '90% of Explained Variance')\n",
    "plt.xlim(0,173)\n",
    "plt.legend(loc='best');\n",
    "\n",
    "print(' Number of Components:',(cumsum < 90).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for balancing target feature with oversampling by SMOTE class of imblearn library \n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=42)\n",
    "X_train,y_train = sm.fit_sample(X_train,y_train)\n",
    "X_test,y_test = sm.fit_sample(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking balance\n",
    "y_train.value_counts().plot(kind='bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('ss',StandardScaler()),\n",
    "                 ('pca',PCA(n_components=126)),\n",
    "                 ('tree_clf',DecisionTreeClassifier(criterion='gini',max_depth=5))])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "pred = pipe.predict(X_test)\n",
    "print(\"Confusion Matrix \\n\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"Classification Reports \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"Roc_auc_score \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_model.plot_feature(pipe[2],X_train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('ss',StandardScaler()),\n",
    "                 ('pca',PCA(n_components=126)),\n",
    "                 ('rf',RandomForestClassifier(n_estimators=100,max_depth=5))])\n",
    "\n",
    "pipe_rf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics of Randam Forest Algorithm\n",
    "pred = pipe_rf.predict(X_test)\n",
    "print(\"Confusion Metrics \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print((\"Classification Report\\n\"))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"Roc_auc_score \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot importance features \n",
    "\n",
    "manage_model.plot_feature(pipe_rf[2],X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([('sc',StandardScaler()),\n",
    "                 ('pca',PCA(n_components=126)),\n",
    "                 ('rf',RandomForestClassifier(random_state=123))])\n",
    "\n",
    "\n",
    "# create the grid parameter\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [5, 8]\n",
    "min_samples_split = [2, 5]\n",
    "min_samples_leaf = [ 5, 10]\n",
    "\n",
    "grid = [{'rf__n_estimators':n_estimators,\n",
    "          'rf__max_depth':max_depth,\n",
    "          'rf__min_samples_split':min_samples_split,\n",
    "          'rf__min_samples_leaf':min_samples_leaf}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch  = GridSearchCV(estimator=pipe,param_grid=grid,scoring='accuracy',cv=3)\n",
    "gridsearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.scorer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining test and train app for featuretool\n",
    "\n",
    "df_app_test['TARGET'] = np.nan\n",
    "df_app_test['SET'] = 'test'\n",
    "df_app_train['SET']='train'\n",
    "\n",
    "app = df_app_train.append(df_app_test,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app.shape)\n",
    "app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model on prepared data which is merged by all dataframe\n",
    "\n",
    "Here , train_bureau.csv and previous_loan_final.csv which are generated on part 1 and part 2 are merged for making whole dataframe named home_credit_final.csv. It is used for final model and prediction client's repayment abilities . Final dataframe has all information of client where are previous loan of another institute(credit bureau) and same institute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive data which is made already from part 1 where it is made by joining application_train , bureau , bureau_balance \n",
    "df_app_bureau = pd.read_csv(\"Data/train_bureau.csv\",)\n",
    "print(df_app_bureau.shape)\n",
    "df_app_bureau = df_app_bureau.iloc[:,1:]\n",
    "df_app_bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from previous loan\n",
    "\n",
    "df_previous_loan = pd.read_csv(\"Data/previou_loan_final.csv\")\n",
    "print(df_previous_loan.shape)\n",
    "df_previous_loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging df_app_bureau and df_previous_loan_fina dataframe for making final dataframe \n",
    "\n",
    "df_home_final = df_app_bureau.merge(df_previous_loan,on='SK_ID_CURR',how='left')\n",
    "print(df_home_final.shape)\n",
    "df_home_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final data of app_bureu and previous \n",
    "\n",
    "df_home_final.to_csv(\"Data/home_credit_final.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete dataframe from memory\n",
    "\n",
    "del df_app_bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retriving from saved  csv file for further steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting final mergred final data \n",
    "\n",
    "df_home_final = pd.read_csv(\"Data/home_credit_final.csv\")\n",
    "print(df_home_final.shape)\n",
    "df_home_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null value \n",
    "\n",
    "manage_df.missing_data_display(df_home_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from original \n",
    "df_home_final_update = df_home_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns which has more than 40% and check null values\n",
    "\n",
    "manage_df.delete_missing_values(df_home_final_update)\n",
    "manage_df.missing_data_display(df_home_final_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "manage_df.missing_data_display(df_home_final_update).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows because it's 6.2% only \n",
    "\n",
    "df_home_final_update.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "manage_df.missing_data_display(df_home_final_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimension of final dataframe \n",
    "\n",
    "print(df_home_final_update.shape)\n",
    "df_home_final_update.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation target between features\n",
    "manage_corr.target_corrs(df_home_final_update)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_corr.Kde_target('previous_NAME_CONTRACT_STATUS_Refused_count_norm',df_home_final_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check balance targete feature \n",
    "df_home_final_update['TARGET'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking sample from population \n",
    "\n",
    "df_home_sample = df_home_final_update.sample(n=60000,random_state=42)\n",
    "\n",
    "# check balance targete feature \n",
    "df_home_sample['TARGET'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between feattures except target \n",
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(df_home_final_update.drop('TARGET',axis = 1).corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for PCA \n",
    "df_pca= df_home_final_update.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "\n",
    "\n",
    "# standerscaling for PCA \n",
    "ss =StandardScaler()\n",
    "df_ss = pd.DataFrame(ss.fit_transform(df_pca),columns=df_pca.columns)\n",
    "print(df_ss.shape)\n",
    "df_ss.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding no of components by PCA algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_ss)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "d = [n for n in range(len(cumsum))]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(d, cumsum, color='red', label = 'Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.axhline(y = 90, color='k', linestyle='--', label = '90% of Explained Variance')\n",
    "plt.xlim(0,300)\n",
    "plt.legend(loc='best')\n",
    "print(\" Number of Components more upto 90%  : \" , (cumsum < 90).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diplay no of important features \n",
    "\n",
    "print(\" Number of Components more upto 90%  : \" , (cumsum < 90).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_home_final_update.shape)\n",
    "df_home_final_update.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparing for model \n",
    "#df_train_bureau =df_train_bureau.iloc[:,1:]\n",
    "#df_train_bureau.head()\n",
    "\n",
    "# taking sample from population \n",
    "\n",
    "df_home_sample = df_home_final_update.sample(n=60000,random_state=42)\n",
    "\n",
    "\n",
    "#df_home_sample = df_home_final_update.copy()\n",
    "\n",
    "# making target and features \n",
    "X = df_home_sample.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "y = df_home_sample['TARGET']\n",
    "\n",
    "# split \n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state =123)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "print(y_train.value_counts().plot(kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making balance target by SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=42)\n",
    "X_train,y_train = sm.fit_sample(X_train,y_train)\n",
    "X_test,y_test = sm.fit_sample(X_test,y_test)\n",
    "\n",
    "# check balance in target \n",
    "\n",
    "fig ,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "y_train.value_counts().plot(kind='bar',ax=ax[0],label='0:repaid 1:notrepaid')\n",
    "y_test.value_counts().plot(kind='bar',ax=ax[1],label='0:repaid 1:notrepaid')\n",
    "ax[0].set_title(\"train target\")\n",
    "ax[1].set_title(\"test target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest \n",
    "\n",
    "# making pipe for RandomForestClassifier\n",
    "\n",
    "pipe_rf = Pipeline([('ss',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=242)),\n",
    "                 ('rf',RandomForestClassifier(random_state =123))])\n",
    "                 \n",
    "pipe_rf.fit(X_train,y_train)   \n",
    "\n",
    "# accuracy\n",
    "print(\"Train score : \", pipe_rf.score(X_train,y_train))\n",
    "print(\"test score :\",pipe_rf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion metrixs for RandomForest \n",
    "\n",
    "pred = pipe_rf.predict(X_test)\n",
    "print(\"Confusion Matrix \")\n",
    "print(\"------------------ \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"\\n Classification Report  \")\n",
    "print(\"---------------------------- \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\" Roc_auc_score :\")\n",
    "print(\"------------------ \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance feature \n",
    "manage_model.plot_feature(pipe_rf[2],X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGradientBoosting Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "pipe_lgbm = Pipeline([('ss',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=242)),\n",
    "                 ('lgbm',lgb.LGBMClassifier())])\n",
    "                 \n",
    "pipe_lgbm.fit(X_train,y_train)   \n",
    "\n",
    "# accuracy\n",
    "print(\"Train score : \", pipe_lgbm.score(X_train,y_train))\n",
    "print(\"test score :\",pipe_lgbm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion metrics for RandomForest \n",
    "\n",
    "pred = pipe_lgbm.predict(X_test)\n",
    "print(\"Confusion Matrix \")\n",
    "print(\"------------------ \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"\\n Classification Report  \")\n",
    "print(\"---------------------------- \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\" Roc_auc_score :\")\n",
    "print(\"------------------ \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance feature \n",
    "manage_model.plot_feature(pipe_lgbm[2],X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_final_update['train_CNT_CHILDREN_mean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check kde plot of most important featuter\n",
    "\n",
    "manage_corr.Kde_target('train_AMT_CREDIT_count',df_home_final_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check kde plot of most important featuter\n",
    "\n",
    "manage_corr.Kde_target('train_CNT_CHILDREN_mean',df_home_final_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes GaussionNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes GaussianNB\n",
    "\n",
    "pipe_naive = Pipeline([('ss',StandardScaler()),\n",
    "                       ('pca',PCA(n_components=242)),\n",
    "                       ('ga',GaussianNB())])\n",
    "                 \n",
    "pipe_naive.fit(X_train,y_train) \n",
    "\n",
    "print(\"Train score : \", pipe_naive.score(X_train,y_train))\n",
    "print(\"test score :\",pipe_naive.score(X_test,y_test))\n",
    "\n",
    "# confusion metrixs for Naive Bayes GaussianNB \n",
    "\n",
    "pred = pipe_naive.predict(X_test)\n",
    "print(\"Confusion Matrix \")\n",
    "print(\"------------------ \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"\\n Classification Report  \")\n",
    "print(\"---------------------------- \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\" Roc_auc_score :\")\n",
    "print(\"------------------ \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada Boosting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "\n",
    "pipe_ada = Pipeline([('ss',StandardScaler()),\n",
    "                     ('pca',PCA(n_components=242)),\n",
    "                     ('ada',AdaBoostClassifier(random_state=123))])\n",
    "                 \n",
    "pipe_ada.fit(X_train,y_train) \n",
    "\n",
    "print(\"Train score : \", pipe_ada.score(X_train,y_train))\n",
    "print(\"test score :\",pipe_ada.score(X_test,y_test))\n",
    "\n",
    "# confusion metrixs for Ada Boosting Algorithm \n",
    "\n",
    "pred = pipe_naive.predict(X_test)\n",
    "print(\"Confusion Matrix \")\n",
    "print(\"------------------ \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"\\n Classification Report  \")\n",
    "print(\"---------------------------- \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\" Roc_auc_score :\")\n",
    "print(\"------------------ \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST \n",
    "\n",
    "pipe_xgb = Pipeline([('ss',StandardScaler()),\n",
    "                     ('pca',PCA(n_components=242)),\n",
    "                     ('xgb',xgb.XGBClassifier(random_state=123))])\n",
    "                 \n",
    "pipe_xgb.fit(X_train,y_train) \n",
    "\n",
    "print(\"Train score : \", pipe_ada.score(X_train,y_train))\n",
    "print(\"test score :\",pipe_ada.score(X_test,y_test))\n",
    "\n",
    "# confusion metrixs for Ada Boosting Algorithm \n",
    "\n",
    "pred = pipe_xgb.predict(X_test)\n",
    "print(\"Confusion Matrix \")\n",
    "print(\"------------------ \\n \")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(\"\\n Classification Report  \")\n",
    "print(\"---------------------------- \\n\")\n",
    "print(classification_report(y_test,pred))\n",
    "print(\" Roc_auc_score :\")\n",
    "print(\"------------------ \\n\")\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of XGBoost\n",
    "\n",
    "plt.figure(figsize = (10,6))  # figure size\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='g',ax =ax , xticklabels= ['Repaid', 'Not_repaid'] ,\n",
    "            yticklabels=['Repaid','Not_repaid'],cmap ='YlGnBu' )\n",
    "\n",
    "# subplot\n",
    "ax.set_xlabel(\"Predicted labels\" , fontsize = 20 );ax.set_ylabel(\"True labels\" , fontsize = 20 )\n",
    "ax.set_title(\"Confusion Matrix\", fontsize = 20)\n",
    "#ax.xaxis.set_ticklabels(['Repaid', 'Not_repaid'], fontsize = 14); \n",
    "#ax.yaxis.set_ticklabels(['Repaid','Not_repaid'], fontsize = 14);\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"Confusion_matrix_xgb.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train data \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree  of XGBoost\n",
    "\n",
    "xgb.plot_tree(pipe_xgb[2])\n",
    "plt.rcParams['figure.figsize']=[200,150]\n",
    "plt.show()\n",
    "plt.savefig(\"xgb_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making pipe line for all algorithm\n",
    "\n",
    "# GaussianNB\n",
    "pipe_naive = Pipeline([('ss',StandardScaler()),\n",
    "                       ('pca',PCA(n_components=242)),\n",
    "                       ('ga',GaussianNB())])\n",
    "\n",
    "#AdaBoostClassifier\n",
    "#pipe_ada = Pipeline([('ss',StandardScaler()),\n",
    "#                 ('ada',AdaBoostClassifier(random_state=123))])\n",
    "\n",
    "#RandomForest \n",
    "pipe_rf = Pipeline([('ss',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=242)),\n",
    "                    ('rf',RandomForestClassifier(random_state =123))])\n",
    "\n",
    "# Lightgbm\n",
    "\n",
    "pipe_lgbm = Pipeline([('ss',StandardScaler()),\n",
    "                      ('pca',PCA(n_components=242)),\n",
    "                      ('lgbm',lgb.LGBMClassifier())])\n",
    "\n",
    "\n",
    "# logistic Regression model \n",
    "pipe_logistic  = Pipeline([('ss',StandardScaler()),\n",
    "                           ('pca',PCA(n_components=242)),\n",
    "                           ('lg',LogisticRegressionCV())])\n",
    "\n",
    "\n",
    "pipelists = [pipe_logistic,pipe_naive,pipe_rf,pipe_lgbm]\n",
    "pipeline_names = ['Logistic Regression','Naive Bayes','RandomForest','LightGredientBoosting Algorithm']\n",
    "\n",
    "\n",
    "\n",
    "# for loop to fit each algorithm\n",
    "for pipe in pipelists:\n",
    "    print(pipe)\n",
    "    pipe.fit(X_train,y_train) \n",
    "    \n",
    "#Compare Accuracies\n",
    "for index,val in enumerate(pipelists):\n",
    "    print(\"%s pipeline test accuracy : %.3f\" %(pipeline_names[index],val.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning and optimization of RandomForest and  LightGBM algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest \n",
    "pipe_rf = Pipeline([('sc',StandardScaler()),\n",
    "                 ('pca',PCA(n_components=242)),\n",
    "                 ('rf',RandomForestClassifier(random_state=123))])\n",
    "\n",
    "\n",
    "# create the grid parameter\n",
    "n_estimators = [100, 300,400]\n",
    "max_depth = [5, 8]\n",
    "min_samples_split = [2, 5,8]\n",
    "min_samples_leaf = [ 5, 10,15]\n",
    "\n",
    "grid = [{'rf__n_estimators':n_estimators,\n",
    "          'rf__max_depth':max_depth,\n",
    "          'rf__min_samples_split':min_samples_split,\n",
    "          'rf__min_samples_leaf':min_samples_leaf}]\n",
    "\n",
    "\n",
    "gridsearch  = GridSearchCV(estimator=pipe_rf,param_grid=grid,scoring='accuracy',cv=3)\n",
    "gridsearch.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter \" )\n",
    "print(\"-----------------\\n\")\n",
    "print(gridsearch.best_params_)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Best Score \")\n",
    "print(\"-----------\\n\")\n",
    "print(gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best paramete and score\n",
    "\n",
    "print(\"Best Parameter \" )\n",
    "print(\"-----------------\\n\")\n",
    "print(gridsearch.best_params_)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Best Score \")\n",
    "print(\"-----------\\n\")\n",
    "print(gridsearch.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with best gridsearch parameter\n",
    "\n",
    "print(\"Best from Grid Search Train Score \")\n",
    "print(\"-----------\\n\")\n",
    "print(gridsearch.best_score_)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "best = gridsearch.best_estimator_\n",
    "y_pred_grid = best.predict(X_test)\n",
    "print(\"Best Test score with gridsearch's best parameter: \")\n",
    "print(\"--------------------------------\\n\")\n",
    "print(accuracy_score(y_pred_grid,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light GBM optimization\n",
    "\n",
    "\n",
    "pipe_lgbm = Pipeline([('ss',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=242)),\n",
    "                 ('lgbm',lgb.LGBMClassifier())])\n",
    "\n",
    "\n",
    "# create the grid parameter\n",
    "#n_estimators = [100, 300,400]\n",
    "#max_depth = [5, 8]\n",
    "#min_samples_split = [2, 5,8]\n",
    "#min_samples_leaf = [ 5, 10,15]\n",
    "\n",
    "grid_para = {'lgbm__boosting_type':['gbdt','goss','dart'],\n",
    "              'lgbm__num_leave':list(range(20,150)),\n",
    "              'lgbm__learning_rate':list(np.logspace(np.log10(0.005),np.log10(0.5),base =10,num=1000)),\n",
    "               'lgbm__subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "               'lgbm__min_child_samples': list(range(20, 500, 5)),\n",
    "               'lgbm__reg_alpha': list(np.linspace(0, 1)),\n",
    "                'lgbm__reg_lambda': list(np.linspace(0, 1)),\n",
    "                'lgbm__colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "                'lgbm__subsample': list(np.linspace(0.5, 1, 100)),\n",
    "                'lgbm__is_unbalance': [True, False]}\n",
    "\n",
    "\n",
    "gridsearch  = GridSearchCV(estimator=pipe_lgbm,param_grid=grid_para,scoring='accuracy',cv=3)\n",
    "gridsearch.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter \" )\n",
    "print(\"-----------------\\n\")\n",
    "print(gridsearch.best_params_)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Best Score \")\n",
    "print(\"-----------\\n\")\n",
    "print(gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light GBM optimization\n",
    "\n",
    "\n",
    "\n",
    "grid_para = {'lgbm__boosting_type':['gbdt','goss','dart'],\n",
    "              'lgbm__num_leave':list(range(20,150)),\n",
    "              'lgbm__learning_rate':list(np.logspace(np.log10(0.005),np.log10(0.5),base =10,num=1000)),\n",
    "               'lgbm__subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "               'lgbm__min_child_samples': list(range(20, 500, 5)),\n",
    "               'lgbm__reg_alpha': list(np.linspace(0, 1)),\n",
    "                'lgbm__reg_lambda': list(np.linspace(0, 1)),\n",
    "                'lgbm__colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "                'lgbm__subsample': list(np.linspace(0.5, 1, 100)),\n",
    "                'lgbm__is_unbalance': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5\n",
    "\n",
    "train_set = lgb.Dataset(data = X_train,label=y_train)\n",
    "test_set =lgb.Dataset(data= X_test,label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "default_params = model.get_params()\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators']\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "default parameter\n",
    "\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,\n",
    "    min_child_weight=0.001,\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,\n",
    "    subsample_freq=0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pipe_lgbm = Pipeline([('ss',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=242)),\n",
    "                 ('lgbm',lgb.LGBMClassifier())])\n",
    "\n",
    "grid_para = {'lgbm__boosting_type':['gbdt','goss','dart'],\n",
    "              'lgbm__num_leave':list(range(20,150)),\n",
    "              'lgbm__learning_rate':list(np.logspace(np.log10(0.005),np.log10(0.5),base =10,num=1000)),\n",
    "               'lgbm__subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "               'lgbm__min_child_samples': list(range(20, 500, 5)),\n",
    "               'lgbm__reg_alpha': list(np.linspace(0, 1)),\n",
    "                'lgbm__reg_lambda': list(np.linspace(0, 1)),\n",
    "                'lgbm__colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "                'lgbm__subsample': list(np.linspace(0.5, 1, 100)),\n",
    "                'lgbm__is_unbalance': [True, False]}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_lgbm,param_grid=grid_para,scoring=\"accuracy\",cv = 5 )                 \n",
    "gs.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = model.get_params()\n",
    "default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate from hyperparameter dict of lightGBM\n",
    "plt.figure(figsize = (14,8))\n",
    "sns.distplot(grid_para['lgbm__learning_rate'],bins=20,kde=False)\n",
    "plt.xlabel(\"Learning Rate \" ,fontsize =14)\n",
    "plt.ylabel(\"Count\" ,fontsize =14)\n",
    "plt.title(\"Learning Rate Distribution \" ,fontsize =14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com =1 \n",
    "for x in grid_para.values():\n",
    "    com *= len(x)\n",
    "print(\"there are {} combination \" .format(com))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using objective function to find best hyperpara meter\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def grid_search(para_grid , max_evals = MAX_EVALS):\n",
    "    \n",
    "    # datframe for store \n",
    "    results = pd.DataFrame(columns =['score','params','iteration'],index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # \n",
    "    \n",
    "    keys, values = zip(*grid_para.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for v in itertools.product(*values):\n",
    "        hyperpara = dict(zip(keys,v))\n",
    "        if hyperpara['boosting_type'] == 'goss' :\n",
    "            hyperpara['subsample'] =1.0\n",
    "        \n",
    "        \n",
    "        #evalute para meter\n",
    "        \n",
    "        eval_result = objective()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key , values = zip(*grid_para.items())\n",
    "for  v in itertools.product(*values):\n",
    "    #print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
